# Stanford CS224W {#stanfordyoutube}

Link to [playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)

#### Contents {-#contents}

[1.1 Why Graphs]  
[1.2 Applications of Graph ML]  
[1.3 Choice of Graph Representation]  
[2.1 Traditional Feature-based Methods: Node]  
[2.2 Traditional Feature-based Methods: Link]  
[2.3 Traditional Feature-based Methods: Graph]  
[3.1 Node Embeddings]  
[3.2 Random Walk Approaches for Node Embeddings]  
[3.3 Embedding Entire Graphs]  
[13.1 Community Detection in Networks]  
[13.2 Network Communities]  
[13.3 Louvain Algorithm]  
[13.4 Detecting Overlapping Communities]  

[blah](#blah)

***

#### 1.1 Why Graphs {-#blah}

Link to [video](https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=2)

* Many types of data that can naturally be represented as graphs
* Explicitly modeling the relational structure allows us to build more accurate models

##### Types of networks and graphs {-}

Sometimes the distinction between these two types (networks and graphs) is blurred.

1. Networks (i.e. natural graphs)

* social networks
* communication networks
* interactions between genes and proteins
* brain connections

2. Graphs as a representation

* similarity networks
* 3d shapes
* particle-based physics simulations

##### Why is it hard {-}

* modern deep learning methods designed for simple sequences and grids (e.g. images)
* networks are complex
  * arbitrary size
  * no fixed node ordering or reference point
  * often dynamic

##### Deep learning in graphs {-}

* want to build neural networks that take a graph as input and make output predictions at the level of:
  * individual nodes
  * edges (i.e. pairs of nodes)
  * new graphs / sub-graphs
  
##### Representation learning {-}

* map nodes to d-dimensional embeddings (using a function that is learned) such that similar nodes in the network are embedded close together

[back to contents](#contents)

***

#### 1.2 Applications of Graph ML {-}

Link to [video](https://www.youtube.com/watch?v=aBHC6xzx9YI&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=3)

##### classic graph ML tasks {-}

* node classification - predicting a property of a node
* edge prediction - predicting whether there are missing edges
* graph classification - categorizing graphs
* clustering - community detection
* graph generation
* graph evolution

##### node-level example: protein folding {-}

* given a sequence of amino acids can we predict the 3d structure of the protein
* in 2020 DeepMind announced AlphaFold which performed this task with high accuracy
* key idea was to represent the amino acids as a spatial graph
  * nodes = amino acids - tried to predict position in space
  * edges = proximity between amino acids

##### edge-level example: recommender systems {-}

* nodes = users and items
* edges = user-item interactions
* want to predict what other items users might be interested in in the future
* task: learn node embeddings such that nodes that are related are closer to nodes that are unrelated

##### edge-level example 2: drug side-effects {-}

* many patients take multiple drugs to treat complex / co-existing diseass
* interactions can lead to side-effects
* task: given a pair of drugs, predict adverse side effects
* nodes = drug or protein
* edge = drug-drug side effects, drug-protein interaction, protein-protein interaction
* want to predict drug-drug interactions

##### subgraph-level example: traffic prediction {-}

* node = road segment
* edge = connectivity between road segments
* predict journey time
* used in google maps

##### graph-level example: drug / antibiotic discovery {-}

* nodes = atoms
* edges = chemical bonds
* predict which molecules should be prioritised for testing

##### graph-level example 2: physics simulation {-}

* nodes = particles
* edge = interactions between particles
* predict positions of particles in future given current positions and velocities

[back to contents](#contents)

***

#### 1.3 Choice of Graph Representation {-}

Link to [video](https://www.youtube.com/watch?v=P-m1Qv6-8cI&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=4)

##### Components of a network {-}

$N$ = nodes / vertices / objects  
$E$ = edges / links  
$G(N, E)$ = graph / network  

##### Types {-}

* undirected
  * collaborations
  * friendships
  
* directed
  * phone calls
  * financial transactons
  * following on Twitter

##### Node degrees {-}

$k_i$ = number of edges adjacet to node $i$

* undirected

$$\overline{k} = \frac{1}{N}\sum_{i=i}^Nk_i=\frac{2E}{N}$$

* directed

$k_i^{in}$ = in-degree number of edges adjacet to node $i$  
$k_i^{out}$ = out-degree number of edges adjacet to node $i$  

$$\overline{k}=\frac{E}{N}$$
$$\overline{k^{in}}=\overline{k^{out}}$$

* bipartite
  * nodes can be divided into two partitions
  * e.g.  
    * authors to papers they authored
    * actors to movies they appeared in
    * recipes to ingredients they contain
  * can project connections between nodes in one partition e.g. if they share at least one node in common from the other partition

##### Representations {-}

**Adjacency matrix** $\bf{A}$    
* $N$ by $N$ matrix with elements $A_{ij}$  
* $A_{ij}$ = 1 if node $i$ connected to node $j$, zero otherwise  
* self-loops on the diagonal
* undirected graphs have symmetrical adjacency matrices (directed graphs might)
* very sparse  

**List of edges**  
* harder to do computations on graph  
  
**Adjaceny list**  
* for every node you store a list of its neighbours
* easier to work with if netwrok is large / sparse  
* can quickly retrieve all neighbours of a given node  

##### Attributes / properties {-}

* weight - can be represented by the $A_{ij}$ in the adjacency matrix
* ranking
* type
* sign
* number of friends in common (i.e. properties depending on the structure of the rest of the graph)

##### Connectivity {-}

* disconnected graph is made up of two or more **connected components**
* adjacency matrix can be written as a block-diagonal matrix

[back to contents](#contents)

***

#### 2.1 Traditional Feature-based Methods: Node {-}

Link to [video](https://www.youtube.com/watch?v=3IS7UhNMQ3U&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=4&pp=iAQB)

* task is to predict node labels

##### Node-level features {-}

* node degree - treats all neighbouring nodes equally  
* node centrality - takes node importance into account - e.g.  
  * eigenvector centrality - uses eigenvector associated with max eigenvalue of $\bf{A}$
  * betweenness centrality - important nodes lie on many shortest paths between other nodes
  * closeness centrality - important nodes have snall shortest path lengths to all other nodes
* clustering coefficient - measures how connected the neighburing nodes are (effectively the number of triangular graphlets touching the node)
* graphlet degree - extends the idea of clustering coefficient - counts number of pre-specfied subgraphs touching the node
  * graphlet = rooted connected non-isomorphic subgraph (lots of different examples)
  * provides a measure of a node's local network topology


[back to contents](#contents)

***

#### 2.2 Traditional Feature-based Methods: Link {-}

Link to [video](https://www.youtube.com/watch?v=4dVwlE9jYxY&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=6)

* task is to predict new links

##### Link-level features {-}

* distance-based features - does not capture the degree of neighbourhood overlap  
* local neighbourhood overlap - the number of neighbouring nodes shared between two nodes
  * common neighbours
  * jaccard's coeffieient
  * adamic-adar index
* global neighbourhood overlap  
  * katz index - count the number of paths of all lengths between a pair of nodes
    * $\bf{P}^{(\bf{K})}_{\bf{u}\bf{v}}=$ number of paths of length $\bf{K}$ between $\bf{u}$ and $\bf{v}$
    * $\bf{P}^{(\bf{K})}=\bf{A}^{\bf{k}}$
    * Sum over all possible path lengths to get the index (where lengths are discounted exponentially)

[back to contents](#contents)

***

#### 2.3 Traditional Feature-based Methods: Graph {-}

Link to [video](https://www.youtube.com/watch?v=buzsHTa4Hgs&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=7)

* want features that characterise the stucture of an entire graph

##### Graph-level features {-}

* kernel methods $K(G_1, G_2)$
  * once the kernel is defined, can use ML models (e.g. kernel SVM) to make predictions
  * graph kernels measure similarity between two graphs
  * key idea: bag-of-words representation of a graph, where 'words' can be:
    * nodes (too simplistic)
    * node degrees 
    * graphlets - i.e. graphlet kernel (don't need to be rooted or connected in this case) very expensive to compute
    * Weisfeiler-Lehman kernel is more computationally efficient

[back to contents](#contents)

***

#### 3.1 Node Embeddings {-}

Link to [video](https://www.youtube.com/watch?v=rMq21iY61SE&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=8)

* goal is to encode nodes such that similarity in the embedding space approximates similarity in the graph
* **encoder** maps from nodes to embeddings - maps each node to a d-dimensional vector
* **decoder** maps from embeddings to similarity scores (e.g. dot product between node embeddings)
* first need to define a similarity function = measure of similarity in the original network
  * lots of ways to define similiarity between nodes (linked, shared neighbours, etc.)
* then need to optimize the parameters of the encoder such that similarity function is as close as possible to similarity as measured by the embedding decoder

##### Shallow encoding {-}

* simplest approach
* not very scalable (too many parameters)
* encoder is just an embedding lookup of matrix $\bf{Z}$ (which must be learned)

$$ENC(v) = \bf{z}_v = \bf{Z} \cdot v$$
where,  
$\bf{Z}$ is a matrix where each column is a node and each row is a dimension of the embedding space - each element represents a parameter which must be optimized
$\bf{v}$ is an indicator vector with a 1 in the column indicating node $v$

[back to contents](#contents)

***

#### 3.2 Random Walk Approaches for Node Embeddings {-}

Link to [video](https://www.youtube.com/watch?v=Xv0wRy66Big&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=8)

##### DeepWalk {-}

* want nodes that are visited in the same random walk to be close together in the embedding space

* want to choose parameters of $\bf{Z}$ which maximise the probability that nodes within a given node's neighbourhood are visited on a random walk from that node

###### Log-likelihood objective {-}

$$= \sum_{u \in V} \log P\left( N_R(u) \; | \; \bf{z}_u \right)$$
$$=\sum_{u \in V} \sum_{v \in N_R(u)}-\log\left(P(v\;|\;\bf{z}_u)\right)$$

$$=\sum_{u \in V} \sum_{v \in N_R(u)}-\log\left(\frac{\exp(\bf{z}_u^T\bf{z}_v)}{\sum_{n \in v} \exp(\bf{z}_u^T\bf{z}_n)}\right)$$

where,  

$P\left(v \; | \; \bf{z}_u\right)$ = probability of visiting node $v$ on random walks starting from node $u$.

$N_R(u)$ = neighbourhood of node $u$ by random walk strategy $R$ (i.e. random walk neighbourhood)  

###### Negative sampling {-}

* rather than all the nodes, sum over a random subset of $k$ nodes  
* sampling probability of each node being proportional to its degree (in practice use $k$ = 5 to 20)  

###### Stochastic gradient descent {-}

* used to minimise the objective function

* like gradient descent, but instead of evaluating gradient over all examples evaluate over a random sample of examples (i.e. a stochastic approximation)

##### node2vec {-}

* flexible notion of network neighbourhood of node $u$ leads to rich embeddings

* use flexible, biased random walks that can trade off between local and global views of the network - i.e. depth-first search (DFS) vs breadth-first search (BFS)

* a biased, second-order random walk

* two parameters used to gend=erate $N_R(u)$

1. $p$ = return parameter (return back to the previous node)

2. $q$ = in-out parameter ('ratio' of BFS vs DFS)

###### algorithm {-}

  * compute random walk probabilities
  * simulate $r$ random walks of length $l$ starting from each node $u$
  * optimize the node2vec objective using stochastic gradient descent

[back to contents](#contents)

***

#### 3.3 Embedding Entire Graphs {-}

Link to [video](https://www.youtube.com/watch?v=eliMLfJeu7A&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=9)

* goal is to embed a sub-graph or an entire graph in an embedding space
* e.g. identify toxic vs non-toxic molecules in the embedding space


[back to contents](#contents)

***

#### 13.1 Community Detection in Networks {-}

Link to [video](https://www.youtube.com/watch?v=KXi4ha79o3s&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=38)

* clustering of nodes based on the structure of the network

##### Social networks example {-}

* socially strong short range edges within communities, redundant in terms of information access
* socially weak long range edges between communities, allow access to information from other parts of the graph

##### Edge overlap {-}

* measures how strong an edge is structurally
* what is the fraction of neighbours that the edge end points have in common
* example using phone calls (=edges) showed positive correlation between edge overlap and the number of phone calls  

[back to contents](#contents)

***

#### 13.2 Network Communities {-}

Link to [video](https://www.youtube.com/watch?v=mJQrtXZT5pw&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=38)

* communities / clusters / groups / modules
  * lots of internal connections
  * few external connections
  
##### Example: Karate club dataset {-}

* club split in two during study
* the two factions show strikingly dense within-community connections

##### Modularity {-}

* measures how well a network is partitioned into communities  
* for a given partitioning of the network $s \in S$:

$$Q \propto \sum_{s \in S} \left[(\# \; edges \; within \; group \; s)-(expected \; \# \; edges \; within \; group \; s)\right]$$

* the expected number of edges between nodes $i$ and $j$ of degrees $k_i$ and $k_j$ is given by  

$$\frac{k_ik_j}{2m}$$
where,

$m$ = the total number of edges in the graph  
$2m$ = the total number of edge end points in the graph  
$k_i$ = the number of out-going edges from node $i$  

E.g. 

$k_1=4$  
$k_2=4$  
$k_1k_2=16$  
$2m=k_1+k_2=8$ ?  
$m=4$  
$\frac{k_ik_j}{2m}=\frac{16}{8}=2$



[back to contents](#contents)

***

#### 13.3 Louvain Algorithm {-}

Link to [video]()

[back to contents](#contents)

***

#### 13.4 Detecting Overlapping Communities {-}

Link to [video]()

[back to contents](#contents)

***

#### {-}

Link to [video]()

[back to contents](#contents)

***

#### {-}

Link to [video]()

[back to contents](#contents)

***

#### {-}

Link to [video]()

[back to contents](#contents)

***

#### {-}

Link to [video]()

[back to contents](#contents)

***

#### {-}

Link to [video]()

[back to contents](#contents)
