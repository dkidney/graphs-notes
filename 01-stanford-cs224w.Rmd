# Stanford CS224W {#stanfordyoutube}

Link to [playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)

#### Contents {-#contents}

[1.1 Why Graphs]  
[1.2 Applications of Graph ML]  
[1.3 Choice of Graph Representation]  

[blah](#blah)

***

#### 1.1 Why Graphs {-}

Link to [video](https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=2)

* Many types of data that can naturally be represented as graphs
* Explicitly modeling the relational structure allows us to build more accurate models

##### Types of networks and graphs {-}

Sometimes the distinction between these two types (networks and graphs) is blurred.

1. Networks (i.e. natural graphs)

* social networks
* communication networks
* interactions between genes and proteins
* brain connections

2. Graphs as a representation

* similarity networks
* 3d shapes
* particle-based physics simulations

##### Why is it hard {-}

* modern deep learning methods designed for simple sequences and grids (e.g. images)
* networks are complex
  * arbitrary size
  * no fixed node ordering or reference point
  * often dynamic

##### Deep learning in graphs {-}

* want to build neural networks that take a graph as input and make output predictions at the level of:
  * individual nodes
  * edges (i.e. pairs of nodes)
  * new graphs / sub-graphs
  
##### Representation learning {-}

* map nodes to d-dimensional embeddings (using a function that is learned) such that similar nodes in the network are embedded close together

[back to contents](#contents)

***

#### 1.2 Applications of Graph ML {-}

Link to [video](https://www.youtube.com/watch?v=aBHC6xzx9YI&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=3)

##### classic graph ML tasks {-}

* node classification - predicting a property of a node
* edge prediction - predicting whether there are missing edges
* graph classification - categorizing graphs
* clustering - community detection
* graph generation
* graph evolution

##### node-level example: protein folding {-}

* given a sequence of amino acids can we predict the 3d structure of the protein
* in 2020 DeepMind announced AlphaFold which performed this task with high accuracy
* key idea was to represent the amino acids as a spatial graph
  * nodes = amino acids - tried to predict position in space
  * edges = proximity between amino acids

##### edge-level example: recommender systems {-}

* nodes = users and items
* edges = user-item interactions
* want to predict what other items users might be interested in in the future
* task: learn node embeddings such that nodes that are related are closer to nodes that are unrelated

##### edge-level example 2: drug side-effects {-}

* many patients take multiple drugs to treat complex / co-existing diseass
* interactions can lead to side-effects
* task: given a pair of drugs, predict adverse side effects
* nodes = drug or protein
* edge = drug-drug side effects, drug-protein interaction, protein-protein interaction
* want to predict drug-drug interactions

##### subgraph-level example: traffic prediction {-}

* node = road segment
* edge = connectivity between road segments
* predict journey time
* used in google maps

##### graph-level example: drug / antibiotic discovery {-}

* nodes = atoms
* edges = chemical bonds
* predict which molecules should be prioritised for testing

##### graph-level example 2: physics simulation {-}

* nodes = particles
* edge = interactions between particles
* predict positions of particles in future given current positions and velocities

[back to contents](#contents)

***

#### 1.3 Choice of Graph Representation {-#blah}

Link to [video](https://www.youtube.com/watch?v=P-m1Qv6-8cI&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=4)

##### Components of a network {-}

$N$ = nodes / vertices / objects  
$E$ = edges / links  
$G(N, E)$ = graph / network  

##### Directed vs Undirected {-}

* undirected
  * collaborations
  * friendships
  
* directed
  * phone calls
  * financial transactons
  * following on Twitter

##### Node degrees {-}

$k_i$ = the number of edges adjacet to node $i$

* undirected

$$\overline{k} = \frac{1}{N}\sum_{i=i}^Nk_i=\frac{2E}{N}$$

* directed

$k_i^{in}$ = the in-degree number of edges adjacet to node $i$  
$k_i^{out}$ = the out-degree number of edges adjacet to node $i$  

$$\overline{k}=\frac{E}{N}$$
$$\overline{k^{in}}=\overline{k^{out}}$$

##### Bipartite graphs {-}

* nodes can be divided into two partitions
* e.g. 
  * authors to papers they authored
  * actors to movies they appeared in
  * recipes to ingredients they contain
* can project connections between nodes in one partition e.g. if they share at least one node in common from the other partition

##### Adjacency matrics {-}

* $N$ by $N$ matrix with elements $A_{ij}$
* $A_{ij}$ = 1 if node $i$ connected to node $j$, zero otherwise
* undirected graphs have symmetrical adjacency matrices (directed graphs might)
* very sparse

[back to contents](#contents)

***

#### 2.1 Traditional Feature-based Methods: Node {-}

Link to [video]()

[back to contents](#contents)

***

#### 2.2 Traditional Feature-based Methods: Link {-}

Link to [video](https://www.youtube.com/watch?v=4dVwlE9jYxY&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=6)

[back to contents](#contents)

***

#### 2.3 Traditional Feature-based Methods: Graph {-}

Link to [video](https://www.youtube.com/watch?v=buzsHTa4Hgs&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=7)

[back to contents](#contents)

***

#### 3.1 Node Embeddings {-}

Link to [video](https://www.youtube.com/watch?v=rMq21iY61SE&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=8)

[back to contents](#contents)

***

####

Link to [video]()

[back to contents](#contents)

***

####

Link to [video]()

[back to contents](#contents)

***

####

Link to [video]()

[back to contents](#contents)

***

####

Link to [video]()

[back to contents](#contents)
