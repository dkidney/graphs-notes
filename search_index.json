[["index.html", "Graphs Notes Links", " Graphs Notes Darren Kidney 2024-02-20 Links Stanford CS224W Machine Learning with Graphs: YouTube playlist "],["stanfordyoutube.html", "Chapter 1 Stanford CS224W", " Chapter 1 Stanford CS224W Link to playlist Contents 1.1 Why Graphs 1.2 Applications of Graph ML 1.3 Choice of Graph Representation 2.1 Traditional Feature-based Methods: Node 2.2 Traditional Feature-based Methods: Link 2.3 Traditional Feature-based Methods: Graph 3.1 Node Embeddings blah 1.1 Why Graphs Link to video Many types of data that can naturally be represented as graphs Explicitly modeling the relational structure allows us to build more accurate models Types of networks and graphs Sometimes the distinction between these two types (networks and graphs) is blurred. Networks (i.e. natural graphs) social networks communication networks interactions between genes and proteins brain connections Graphs as a representation similarity networks 3d shapes particle-based physics simulations Why is it hard modern deep learning methods designed for simple sequences and grids (e.g. images) networks are complex arbitrary size no fixed node ordering or reference point often dynamic Deep learning in graphs want to build neural networks that take a graph as input and make output predictions at the level of: individual nodes edges (i.e. pairs of nodes) new graphs / sub-graphs Representation learning map nodes to d-dimensional embeddings (using a function that is learned) such that similar nodes in the network are embedded close together back to contents 1.2 Applications of Graph ML Link to video classic graph ML tasks node classification - predicting a property of a node edge prediction - predicting whether there are missing edges graph classification - categorizing graphs clustering - community detection graph generation graph evolution node-level example: protein folding given a sequence of amino acids can we predict the 3d structure of the protein in 2020 DeepMind announced AlphaFold which performed this task with high accuracy key idea was to represent the amino acids as a spatial graph nodes = amino acids - tried to predict position in space edges = proximity between amino acids edge-level example: recommender systems nodes = users and items edges = user-item interactions want to predict what other items users might be interested in in the future task: learn node embeddings such that nodes that are related are closer to nodes that are unrelated edge-level example 2: drug side-effects many patients take multiple drugs to treat complex / co-existing diseass interactions can lead to side-effects task: given a pair of drugs, predict adverse side effects nodes = drug or protein edge = drug-drug side effects, drug-protein interaction, protein-protein interaction want to predict drug-drug interactions subgraph-level example: traffic prediction node = road segment edge = connectivity between road segments predict journey time used in google maps graph-level example: drug / antibiotic discovery nodes = atoms edges = chemical bonds predict which molecules should be prioritised for testing graph-level example 2: physics simulation nodes = particles edge = interactions between particles predict positions of particles in future given current positions and velocities back to contents 1.3 Choice of Graph Representation Link to video Components of a network \\(N\\) = nodes / vertices / objects \\(E\\) = edges / links \\(G(N, E)\\) = graph / network Types undirected collaborations friendships directed phone calls financial transactons following on Twitter Node degrees \\(k_i\\) = number of edges adjacet to node \\(i\\) undirected \\[\\overline{k} = \\frac{1}{N}\\sum_{i=i}^Nk_i=\\frac{2E}{N}\\] directed \\(k_i^{in}\\) = in-degree number of edges adjacet to node \\(i\\) \\(k_i^{out}\\) = out-degree number of edges adjacet to node \\(i\\) \\[\\overline{k}=\\frac{E}{N}\\] \\[\\overline{k^{in}}=\\overline{k^{out}}\\] bipartite nodes can be divided into two partitions e.g.  authors to papers they authored actors to movies they appeared in recipes to ingredients they contain can project connections between nodes in one partition e.g. if they share at least one node in common from the other partition Representations Adjacency matrix \\(\\bf{A}\\) * \\(N\\) by \\(N\\) matrix with elements \\(A_{ij}\\) * \\(A_{ij}\\) = 1 if node \\(i\\) connected to node \\(j\\), zero otherwise * self-loops on the diagonal * undirected graphs have symmetrical adjacency matrices (directed graphs might) * very sparse List of edges * harder to do computations on graph Adjaceny list * for every node you store a list of its neighbours * easier to work with if netwrok is large / sparse * can quickly retrieve all neighbours of a given node Attributes / properties weight - can be represented by the \\(A_{ij}\\) in the adjacency matrix ranking type sign number of friends in common (i.e. properties depending on the structure of the rest of the graph) Connectivity disconnected graph is made up of two or more connected components adjacency matrix can be written as a block-diagonal matrix back to contents 2.1 Traditional Feature-based Methods: Node Link to video task is to predict node labels Node-level features node degree - treats all neighbouring nodes equally node centrality - takes node importance into account - e.g.  eigenvector centrality - uses eigenvector associated with max eigenvalue of \\(\\bf{A}\\) betweenness centrality - important nodes lie on many shortest paths between other nodes closeness centrality - important nodes have snall shortest path lengths to all other nodes clustering coefficient - measures how connected the neighburing nodes are (effectively the number of triangular graphlets touching the node) graphlet degree - extends the idea of clustering coefficient - counts number of pre-specfied subgraphs touching the node graphlet = rooted connected non-isomorphic subgraph (lots of different examples) provides a measure of a node’s local network topology back to contents 2.2 Traditional Feature-based Methods: Link Link to video task is to predict new links Link-level features back to contents 2.3 Traditional Feature-based Methods: Graph Link to video back to contents 3.1 Node Embeddings Link to video back to contents 1.0.0.1 Link to video back to contents 1.0.0.2 Link to video back to contents 1.0.0.3 Link to video back to contents 1.0.0.4 Link to video back to contents "],["community-detection.html", "Chapter 2 Community detection 2.1 Louvain", " Chapter 2 Community detection 2.1 Louvain "]]
