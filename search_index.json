[["index.html", "Graphs Notes Links", " Graphs Notes Darren Kidney 2024-02-22 Links Stanford CS224W Machine Learning with Graphs: YouTube playlist "],["stanfordyoutube.html", "Chapter 1 Stanford CS224W", " Chapter 1 Stanford CS224W Link to playlist Contents 1.1 Why Graphs 1.2 Applications of Graph ML 1.3 Choice of Graph Representation 2.1 Traditional Feature-based Methods: Node 2.2 Traditional Feature-based Methods: Link 2.3 Traditional Feature-based Methods: Graph 3.1 Node Embeddings 3.2 Random Walk Approaches for Node Embeddings 3.3 Embedding Entire Graphs blah 1.1 Why Graphs Link to video Many types of data that can naturally be represented as graphs Explicitly modeling the relational structure allows us to build more accurate models Types of networks and graphs Sometimes the distinction between these two types (networks and graphs) is blurred. Networks (i.e. natural graphs) social networks communication networks interactions between genes and proteins brain connections Graphs as a representation similarity networks 3d shapes particle-based physics simulations Why is it hard modern deep learning methods designed for simple sequences and grids (e.g. images) networks are complex arbitrary size no fixed node ordering or reference point often dynamic Deep learning in graphs want to build neural networks that take a graph as input and make output predictions at the level of: individual nodes edges (i.e. pairs of nodes) new graphs / sub-graphs Representation learning map nodes to d-dimensional embeddings (using a function that is learned) such that similar nodes in the network are embedded close together back to contents 1.2 Applications of Graph ML Link to video classic graph ML tasks node classification - predicting a property of a node edge prediction - predicting whether there are missing edges graph classification - categorizing graphs clustering - community detection graph generation graph evolution node-level example: protein folding given a sequence of amino acids can we predict the 3d structure of the protein in 2020 DeepMind announced AlphaFold which performed this task with high accuracy key idea was to represent the amino acids as a spatial graph nodes = amino acids - tried to predict position in space edges = proximity between amino acids edge-level example: recommender systems nodes = users and items edges = user-item interactions want to predict what other items users might be interested in in the future task: learn node embeddings such that nodes that are related are closer to nodes that are unrelated edge-level example 2: drug side-effects many patients take multiple drugs to treat complex / co-existing diseass interactions can lead to side-effects task: given a pair of drugs, predict adverse side effects nodes = drug or protein edge = drug-drug side effects, drug-protein interaction, protein-protein interaction want to predict drug-drug interactions subgraph-level example: traffic prediction node = road segment edge = connectivity between road segments predict journey time used in google maps graph-level example: drug / antibiotic discovery nodes = atoms edges = chemical bonds predict which molecules should be prioritised for testing graph-level example 2: physics simulation nodes = particles edge = interactions between particles predict positions of particles in future given current positions and velocities back to contents 1.3 Choice of Graph Representation Link to video Components of a network \\(N\\) = nodes / vertices / objects \\(E\\) = edges / links \\(G(N, E)\\) = graph / network Types undirected collaborations friendships directed phone calls financial transactons following on Twitter Node degrees \\(k_i\\) = number of edges adjacet to node \\(i\\) undirected \\[\\overline{k} = \\frac{1}{N}\\sum_{i=i}^Nk_i=\\frac{2E}{N}\\] directed \\(k_i^{in}\\) = in-degree number of edges adjacet to node \\(i\\) \\(k_i^{out}\\) = out-degree number of edges adjacet to node \\(i\\) \\[\\overline{k}=\\frac{E}{N}\\] \\[\\overline{k^{in}}=\\overline{k^{out}}\\] bipartite nodes can be divided into two partitions e.g.  authors to papers they authored actors to movies they appeared in recipes to ingredients they contain can project connections between nodes in one partition e.g. if they share at least one node in common from the other partition Representations Adjacency matrix \\(\\bf{A}\\) * \\(N\\) by \\(N\\) matrix with elements \\(A_{ij}\\) * \\(A_{ij}\\) = 1 if node \\(i\\) connected to node \\(j\\), zero otherwise * self-loops on the diagonal * undirected graphs have symmetrical adjacency matrices (directed graphs might) * very sparse List of edges * harder to do computations on graph Adjaceny list * for every node you store a list of its neighbours * easier to work with if netwrok is large / sparse * can quickly retrieve all neighbours of a given node Attributes / properties weight - can be represented by the \\(A_{ij}\\) in the adjacency matrix ranking type sign number of friends in common (i.e. properties depending on the structure of the rest of the graph) Connectivity disconnected graph is made up of two or more connected components adjacency matrix can be written as a block-diagonal matrix back to contents 2.1 Traditional Feature-based Methods: Node Link to video task is to predict node labels Node-level features node degree - treats all neighbouring nodes equally node centrality - takes node importance into account - e.g.  eigenvector centrality - uses eigenvector associated with max eigenvalue of \\(\\bf{A}\\) betweenness centrality - important nodes lie on many shortest paths between other nodes closeness centrality - important nodes have snall shortest path lengths to all other nodes clustering coefficient - measures how connected the neighburing nodes are (effectively the number of triangular graphlets touching the node) graphlet degree - extends the idea of clustering coefficient - counts number of pre-specfied subgraphs touching the node graphlet = rooted connected non-isomorphic subgraph (lots of different examples) provides a measure of a node’s local network topology back to contents 2.2 Traditional Feature-based Methods: Link Link to video task is to predict new links Link-level features distance-based features - does not capture the degree of neighbourhood overlap local neighbourhood overlap - the number of neighbouring nodes shared between two nodes common neighbours jaccard’s coeffieient adamic-adar index global neighbourhood overlap katz index - count the number of paths of all lengths between a pair of nodes \\(\\bf{P}^{(\\bf{K})}_{\\bf{u}\\bf{v}}=\\) number of paths of length \\(\\bf{K}\\) between \\(\\bf{u}\\) and \\(\\bf{v}\\) \\(\\bf{P}^{(\\bf{K})}=\\bf{A}^{\\bf{k}}\\) Sum over all possible path lengths to get the index (where lengths are discounted exponentially) back to contents 2.3 Traditional Feature-based Methods: Graph Link to video want features that characterise the stucture of an entire graph Graph-level features kernel methods \\(K(G_1, G_2)\\) once the kernel is defined, can use ML models (e.g. kernel SVM) to make predictions graph kernels measure similarity between two graphs key idea: bag-of-words representation of a graph, where ‘words’ can be: nodes (too simplistic) node degrees graphlets - i.e. graphlet kernel (don’t need to be rooted or connected in this case) very expensive to compute Weisfeiler-Lehman kernel is more computationally efficient back to contents 3.1 Node Embeddings Link to video goal is to encode nodes such that similarity in the embedding space approximates similarity in the graph encoder maps from nodes to embeddings - maps each node to a d-dimensional vector decoder maps from embeddings to similarity scores (e.g. dot product between node embeddings) first need to define a similarity function = measure of similarity in the original network lots of ways to define similiarity between nodes (linked, shared neighbours, etc.) then need to optimize the parameters of the encoder such that similarity function is as close as possible to similarity as measured by the embedding decoder Shallow encoding simplest approach not very scalable (too many parameters) encoder is just an embedding lookup of matrix \\(\\bf{Z}\\) (which must be learned) \\[ENC(v) = \\bf{z}_v = \\bf{Z} \\cdot v\\] where, \\(\\bf{Z}\\) is a matrix where each column is a node and each row is a dimension of the embedding space - each element represents a parameter which must be optimized \\(\\bf{v}\\) is an indicator vector with a 1 in the column indicating node \\(v\\) back to contents 3.2 Random Walk Approaches for Node Embeddings Link to video DeepWalk want nodes that are visited in the same random walk to be close together in the embedding space want to choose parameters of \\(\\bf{Z}\\) which maximise the probability that nodes within a given node’s neighbourhood are visited on a random walk from that node Log-likelihood objective \\[= \\sum_{u \\in V} \\log P\\left( N_R(u) \\; | \\; \\bf{z}_u \\right)\\] \\[=\\sum_{u \\in V} \\sum_{v \\in N_R(u)}-\\log\\left(P(v\\;|\\;\\bf{z}_u)\\right)\\] \\[=\\sum_{u \\in V} \\sum_{v \\in N_R(u)}-\\log\\left(\\frac{\\exp(\\bf{z}_u^T\\bf{z}_v)}{\\sum_{n \\in v} \\exp(\\bf{z}_u^T\\bf{z}_n)}\\right)\\] where, \\(P\\left(v \\; | \\; \\bf{z}_u\\right)\\) = probability of visiting node \\(v\\) on random walks starting from node \\(u\\). \\(N_R(u)\\) = neighbourhood of node \\(u\\) by random walk strategy \\(R\\) (i.e. random walk neighbourhood) Negative sampling rather than all the nodes, sum over a random subset of \\(k\\) nodes sampling probability of each node being proportional to its degree (in practice use \\(k\\) = 5 to 20) Stochastic gradient descent used to minimise the objective function like gradient descent, but instead of evaluating gradient over all examples evaluate over a random sample of examples (i.e. a stochastic approximation) node2vec flexible notion of network neighbourhood of node \\(u\\) leads to rich embeddings use flexible, biased random walks that can trade off between local and global views of the network - i.e. depth-first search (DFS) vs breadth-first search (BFS) a biased, second-order random walk two parameters used to gend=erate \\(N_R(u)\\) \\(p\\) = return parameter (return back to the previous node) \\(q\\) = in-out parameter (‘ratio’ of BFS vs DFS) algorithm compute random walk probabilities simulate \\(r\\) random walks of length \\(l\\) starting from each node \\(u\\) optimize the node2vec objective using stochastic gradient descent back to contents 3.3 Embedding Entire Graphs Link to video back to contents Link to video back to contents Link to video back to contents "],["community-detection.html", "Chapter 2 Community detection 2.1 Louvain", " Chapter 2 Community detection 2.1 Louvain "]]
